# Efficient Guardrailing via Reasoning Distillation

This repository contains the training and evaluation scripts for the paper **"Efficient Guardrailing for the Ukrainian LLM via Reasoning Distillation"**.

It provides the implementation of efficient encoder-based guardrails (Ukr-RoBERTa, mDeBERTa) comparing standard fine-tuning against two proposed distillation methods:
- **Adversarial Contrastive Distillation (ACD)**
- **Rationale Alignment Distillation (RAD)**

## ⚠️ Confidentiality Note
Due to security considerations and non-disclosure agreements, the raw datasets, processing pipelines, and synthetic data generation tools (including the teacher model prompts for hard negatives and rationales) are **not included** in this repository. Only the model training and benchmarking code is provided for reproducibility of the methodology.

## Repository Structure

```text
.
├── 3_benchmark_models.ipynb      # Evaluation of baseline Guardrails (LlamaGuard, ShieldGemma) vs Encoders
├── 4_finetune_bert_models.ipynb  # Standard Fine-tuning baseline (Ukr-RoBERTa, mDeBERTa)
├── 6_finetune_contrastive.ipynb  # Implementation of Adversarial Contrastive Distillation (ACD)
└── 8_finetune_reasoning.ipynb    # Implementation of Rationale Alignment Distillation (RAD)
```

## Methods Implemented

1. **Standard Fine-tuning**: Direct training on the classification task.
2. **ACD (Adversarial Contrastive Distillation)**: Uses Triplet Margin Loss with hard negatives to improve decision boundaries.
3. **RAD (Rationale Alignment Distillation)**: Aligns the encoder's latent space with natural language rationales generated by a Teacher LLM.
